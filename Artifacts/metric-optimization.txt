# File: /backend/monitoring/optimizations.py
# Description: Performance optimizations for metric processing

from typing import Dict, List, Optional
import asyncio
from datetime import datetime, timedelta
import numpy as np
from pymongo import MongoClient, UpdateOne, IndexModel, ASCENDING, DESCENDING
import redis
from concurrent.futures import ThreadPoolExecutor
import pandas as pd
from io import BytesIO
import pickle

class MetricOptimizer:
    def __init__(
        self,
        mongo_uri: str,
        redis_host: str = "localhost",
        redis_port: int = 6379
    ):
        # Initialize MongoDB
        self.client = MongoClient(mongo_uri)
        self.db = self.client.geaux_academy
        
        # Initialize Redis for caching
        self.redis = redis.Redis(
            host=redis_host,
            port=redis_port,
            decode_responses=True
        )
        
        # Initialize thread pool for parallel processing
        self.executor = ThreadPoolExecutor(max_workers=4)
        
        # Setup database indexes
        self._setup_indexes()

    def _setup_indexes(self):
        """Setup optimal indexes for metric queries"""
        indexes = [
            IndexModel([("timestamp", DESCENDING)]),
            IndexModel([("agent_type", ASCENDING), ("timestamp", DESCENDING)]),
            IndexModel([
                ("agent_type", ASCENDING),
                ("metric_type", ASCENDING),
                ("timestamp", DESCENDING)
            ]),
            IndexModel([("aggregated_at", DESCENDING)])
        ]
        
        self.db.metrics.create_indexes(indexes)
        self.db.aggregated_metrics.create_indexes(indexes)

    async def process_metrics_batch(
        self,
        metrics: List[Dict],
        batch_size: int = 1000
    ):
        """Process metrics in optimized batches"""
        if not metrics:
            return
            
        # Convert to DataFrame for efficient processing
        df = pd.DataFrame(metrics)
        
        # Process in batches
        batches = np.array_split(df, max(1, len(df) // batch_size))
        
        async def process_batch(batch):
            # Calculate basic statistics
            stats = {
                'mean': batch['value'].mean(),
                'std': batch['value'].std(),
                'min': batch['value'].min(),
                'max': batch['value'].max()
            }
            
            # Detect anomalies using Z-score
            z_scores = np.abs((batch['value'] - stats['mean']) / stats['std'])
            anomalies = batch[z_scores > 3]
            
            # Prepare database operations
            operations = []
            
            # Insert metrics
            operations.extend([
                UpdateOne(
                    {
                        'timestamp': row['timestamp'],
                        'agent_type': row['agent_type'],
                        'metric_type': row['metric_type']
                    },
                    {'$set': row.to_dict()},
                    upsert=True
                )
                for _, row in batch.iterrows()
            ])
            
            # Flag anomalies
            if not anomalies.empty:
                operations.extend([
                    UpdateOne(
                        {
                            'timestamp': row['timestamp'],
                            'agent_type': row['agent_type'],
                            'metric_type': row['metric_type']
                        },
                        {'$set': {'is_anomaly': True}},
                        upsert=True
                    )
                    for _, row in anomalies.iterrows()
                ])
            
            # Execute database operations
            if operations:
                await self.db.metrics.bulk_write(operations)
            
            return stats, len(anomalies)
        
        # Process batches concurrently
        tasks = [process_batch(batch) for batch in batches]
        results = await asyncio.gather(*tasks)
        
        # Aggregate results
        total_stats = {
            'processed': len(metrics),
            'anomalies': sum(r[1] for r in results),
            'batch_stats': [r[0] for r in results]
        }
        
        return total_stats

    async def cache_recent_metrics(self, time_window: int = 300):
        """Cache recent metrics in Redis for fast access"""
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(seconds=time_window)
        
        # Fetch recent metrics
        metrics = await self.db